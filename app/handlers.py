from aiogram import F, Router
from aiogram.types import Message
from aiogram.filters import CommandStart
from aiogram.fsm.state import State, StatesGroup
from aiogram.fsm.context import FSMContext
from loguru import logger

from app.generators import gpt
from logger import file_logger
from app import cmd_message
from app.count_token import count_tokens
import app.keyboards as kb
from app.split_text import split_text


"""
175 —Ä—É–±–ª–µ–π (300 000 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ gpt-4o + 1 000 000 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ gpt-4o-mini)
–ü—Ä–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –≤ 299, –º–æ—Ä–∂–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è –æ–∫–æ–ª–æ 124 —Ä—É–±–ª—è –∑–∞ –æ–¥–Ω—É –ø–æ–¥–ø–∏—Å–∫—É
"""


router = Router()


class Generate(StatesGroup):
    selecting_model = State()           # –°–æ—Å—Ç–æ—è–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
    text_input = State()                # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    waiting_for_response = State()      # –û–∂–∏–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ gpt


@logger.catch
@router.message(CommandStart())
async def cmd_start(message: Message, state: FSMContext):

    file_logger()

    try:
        await message.answer(cmd_message.start_message, reply_markup=kb.main)
        await state.clear()  # –û—á–∏—Å—Ç–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        await state.set_state(Generate.selecting_model)  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
    except Exception as err:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–≤–æ–¥–µ –∫–æ–º–∞–Ω–¥—ã /start: {err}")
        await message.answer(cmd_message.error_message)


@logger.catch
@router.message(F.text == "–ü–æ–º–µ–Ω—è—Ç—å –º–æ–¥–µ–ª—å gpt")
async def change_gpt_model(message: Message, state: FSMContext):
    file_logger()
    try:
        await message.answer("–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å gpt:", reply_markup=kb.main)
        await state.set_state(Generate.selecting_model)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ –≤—ã–±–æ—Ä—É –º–æ–¥–µ–ª–∏
    except Exception as err:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–º–µ–Ω–µ –º–æ–¥–µ–ª–∏ gpt: {err}")
        await message.answer(cmd_message.error_message)


@logger.catch
@router.message(F.text == "–°–±—Ä–æ—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
async def reset_context(message: Message, state: FSMContext):
    file_logger()
    telegram_id = message.from_user.id
    try:
        from app.generators import message_history
        message_history[telegram_id] = []
        await message.reply(cmd_message.reset_context_message)
    except Exception as err:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±—Ä–æ—Å–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {err}")
        await message.answer(cmd_message.error_message)


@logger.catch
@router.message(F.text.in_(["–ú–æ–¥–µ–ª—å 4-o", "–ú–æ–¥–µ–ª—å 4-o-mini"]))
async def select_model(message: Message, state: FSMContext):
    file_logger()
    model_mapping = {
        "–ú–æ–¥–µ–ª—å 4-o": "gpt-4o",
        "–ú–æ–¥–µ–ª—å 4-o-mini": "gpt-4o-mini"
    }
    model = model_mapping.get(message.text)
    
    await state.update_data(model=model)
    await state.set_state(Generate.text_input)

    await message.answer(f"–í—ã –≤—ã–±—Ä–∞–ª–∏ {model}. –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:", reply_markup=await kb.change_model(model))


@logger.catch
@router.message(Generate.text_input)
async def process_generation(message: Message, state: FSMContext):
    file_logger()

    telegram_id = message.from_user.id

    # –¢—É—Ç –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Å—Ç–∞—Ç—É—Å –ø–æ–¥–ø–∏—Å–∫–∏
    # –ù–æ –ø–æ–∫–∞ —á—Ç–æ —Ç–æ–ª—å–∫–æ —Ç—É–ø–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ telegram_id

    if telegram_id not in [2050793273, 857805093]:
        await message.answer("–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–∞–º –æ—Ç–∫–∞–∑–∞–Ω–æ –≤ –¥–æ—Å—Ç—É–ø–µ, —Å–∫–æ—Ä–æ –±–æ—Ç –≤—ã–π–¥–µ—Ç –≤ –æ–±—â–µ–µ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ!")
        return


    data = await state.get_data()
    model = data.get("model")
    user_input = message.text

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    current_state = await state.get_state()

    if current_state == Generate.waiting_for_response.state:
        await message.reply("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–∂–¥–∏—Ç–µ—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.")
        return

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è
    await state.set_state(Generate.waiting_for_response)

    await message.reply(f"‚ú® –ú–æ–¥–µ–ª—å: {model}. –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è: –≤—Å–µ–≥–æ 5-19 —Å–µ–∫—É–Ω–¥! ‚è±üöÄ\n–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ‚ú®")

    try:
        response = await gpt(user_input, model, telegram_id)
    except Exception as err:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ gpt: {err}")
        await message.answer(cmd_message.error_message)
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è –≤–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞
        await state.set_state(Generate.text_input)
        return

    try:
        # –†–∞–∑–¥–µ–ª—è–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ 4096 —Å–∏–º–≤–æ–ª–æ–≤
        response_parts = split_text(response)
        for part in response_parts:
            await message.reply(
                f"–í–∞—à –æ—Ç–≤–µ—Ç, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é {model}:\n\n{part}\n\n–ö–æ–ª-–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {count_tokens(user_input + part)}", 
                parse_mode="Markdown",
                reply_markup=await kb.change_model(model)  # –ö–Ω–æ–ø–∫–∞ –¥–ª—è —Å–º–µ–Ω—ã –º–æ–¥–µ–ª–∏
            )
        logger.info("–û—Ç–≤–µ—Ç gpt –ø–æ–ª—É—á–µ–Ω –∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –æ–∂–∏–¥–∞–Ω–∏—è –≤–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞
        await state.set_state(Generate.text_input)
    except Exception as err:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {err}")
        await message.reply(cmd_message.error_message)
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è –≤–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞
        await state.set_state(Generate.text_input)
        return


@logger.catch
@router.message(F.text)
async def error_handling(message: Message, state: FSMContext):
    current_state = await state.get_state()

    if current_state == Generate.waiting_for_response.state:
        await message.reply("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–∂–¥–∏—Ç–µ—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.")
    elif current_state == Generate.text_input.state:
        await process_generation(message, state)
    else:
        await message.answer("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å gpt", reply_markup=kb.main)
