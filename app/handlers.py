from aiogram import F, Router
from aiogram.types import Message
from aiogram.filters import CommandStart
from aiogram.fsm.state import State, StatesGroup
from aiogram.fsm.context import FSMContext
from loguru import logger

from app.generators import gpt
from logger import file_logger
from . import cmd_message
from .count_token import count_tokens
import app.keyboards as kb
from .split_text import split_text


router = Router()

class Generate(StatesGroup):
    selecting_model = State()  # –°–æ—Å—Ç–æ—è–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
    text_input = State()       # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    waiting_for_response = State()


@logger.catch
@router.message(CommandStart())
async def cmd_start(message: Message, state: FSMContext):

    file_logger()

    try:
        await message.answer(cmd_message.start, reply_markup=kb.main)
        await state.clear()  # –û—á–∏—Å—Ç–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        await state.set_state(Generate.selecting_model)  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
    except Exception as err:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–≤–æ–¥–µ –∫–æ–º–∞–Ω–¥—ã /start: {err}")
        await message.answer(cmd_message.error)


@logger.catch
@router.message(F.text == "–ü–æ–º–µ–Ω—è—Ç—å –º–æ–¥–µ–ª—å gpt")
async def change_gpt_model(message: Message, state: FSMContext):
    file_logger()
    try:
        await message.answer("–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å gpt:", reply_markup=kb.main)
        await state.set_state(Generate.selecting_model)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ –≤—ã–±–æ—Ä—É –º–æ–¥–µ–ª–∏
    except Exception as err:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–º–µ–Ω–µ –º–æ–¥–µ–ª–∏ gpt: {err}")
        await message.answer(cmd_message.error)


@logger.catch
@router.message(F.text.in_(["–ú–æ–¥–µ–ª—å 4-o", "–ú–æ–¥–µ–ª—å 4-o-mini"]))
async def select_model(message: Message, state: FSMContext):
    file_logger()
    model_mapping = {
        "–ú–æ–¥–µ–ª—å 4-o": "gpt-4o",
        "–ú–æ–¥–µ–ª—å 4-o-mini": "gpt-4o-mini"
    }
    model = model_mapping.get(message.text)
    
    await state.update_data(model=model)
    await state.set_state(Generate.text_input)

    await message.answer(f"–í—ã –≤—ã–±—Ä–∞–ª–∏ {model}. –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:", reply_markup=kb.change_model)


@router.message(Generate.text_input)
async def process_generation(message: Message, state: FSMContext):

    file_logger()

    telegram_id = message.from_user.id

    # if telegram_id not in [2050793273, 857805093]:
    #     await message.answer("–ò–¥–∏ –Ω–∞—Ö—É–π, –¥–æ–ª–±–∞—ë–±!")
    #     return

    if telegram_id not in [857805093]:
        await message.answer("–ò–¥–∏ –Ω–∞—Ö—É–π, –¥–æ–ª–±–∞—ë–±!")
        return


    data = await state.get_data()
    model = data.get("model")
    user_input = message.text

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    current_state = await state.get_state()

    if current_state == Generate.waiting_for_response.state:
        await message.reply("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–∂–¥–∏—Ç–µ—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.")
        return

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è
    await state.set_state(Generate.waiting_for_response)

    await message.reply(f"‚ú® –ú–æ–¥–µ–ª—å: {model}. –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è: –≤—Å–µ–≥–æ 5-19 —Å–µ–∫—É–Ω–¥! ‚è±üöÄ\n–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ‚ú®")

    try:
        response = await gpt(user_input, model)
    except Exception as err:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ gpt: {err}")
        await message.answer(cmd_message.error)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è –≤–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞
        await state.set_state(Generate.text_input)
        return

    try:
        # –†–∞–∑–¥–µ–ª—è–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ 4096 —Å–∏–º–≤–æ–ª–æ–≤
        response_parts = split_text(response)
        for part in response_parts:
            await message.reply(
                f"–í–∞—à –æ—Ç–≤–µ—Ç, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é {model}:\n\n{part}\n\n–ö–æ–ª-–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {count_tokens(user_input + part)}", 
                parse_mode="Markdown",
                reply_markup=kb.change_model  # –ö–Ω–æ–ø–∫–∞ –¥–ª—è —Å–º–µ–Ω—ã –º–æ–¥–µ–ª–∏
            )
        logger.info("–û—Ç–≤–µ—Ç gpt –ø–æ–ª—É—á–µ–Ω –∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –æ–∂–∏–¥–∞–Ω–∏—è –≤–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞
        await state.set_state(Generate.text_input)
    except Exception as err:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {err}")
        await message.reply(cmd_message.error)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è –≤–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞
        await state.set_state(Generate.text_input)
        return


@router.message(F.text)
async def error_handling(message: Message, state: FSMContext):
    current_state = await state.get_state()

    if current_state == Generate.waiting_for_response.state:
        await message.reply("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–∂–¥–∏—Ç–µ—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.")
    elif current_state == Generate.text_input.state:
        await process_generation(message, state)
    else:
        await message.answer("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å gpt", reply_markup=kb.main)
