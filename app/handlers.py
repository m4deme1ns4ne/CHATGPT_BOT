from aiogram import F, Router, Bot
from aiogram.types import Message
from aiogram.filters import CommandStart, Command
from aiogram.fsm.state import State, StatesGroup
from aiogram.fsm.context import FSMContext
from loguru import logger

from app.generators import gpt
from logger import file_logger
from app import cmd_message
from app.count_token import count_tokens
import app.keyboards as kb
from app.split_text import split_text
from .database.db import clear_message_history
from app.call_count_gpt import count_calls
from .database.redis import check_time_spacing_between_messages, del_redis_id
from .calculate_message_length import calculate_message_length
from .escape_markdown import escape_markdown


router = Router()

file_logger()


class Generate(StatesGroup):
    selecting_model = State()           # –°–æ—Å—Ç–æ—è–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
    text_input = State()                # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    waiting_for_response = State()      # –û–∂–∏–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ gpt


@logger.catch
@router.message(CommandStart())
async def cmd_start(message: Message, state: FSMContext):
    try:
        await message.answer(cmd_message.start_message, reply_markup=kb.main)
        await state.clear()  # –û—á–∏—Å—Ç–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        await state.set_state(Generate.selecting_model)  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
    except Exception as err:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–≤–æ–¥–µ –∫–æ–º–∞–Ω–¥—ã /start: {err}")
        await message.answer(cmd_message.error_message)


@logger.catch
@router.message(F.text == "–ü–æ–º–µ–Ω—è—Ç—å –º–æ–¥–µ–ª—å gpt")
async def change_gpt_model(message: Message, state: FSMContext):
    try:
        await message.answer("–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å gpt:", reply_markup=kb.main)
        await state.set_state(Generate.selecting_model)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ –≤—ã–±–æ—Ä—É –º–æ–¥–µ–ª–∏
    except Exception as err:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–º–µ–Ω–µ –º–æ–¥–µ–ª–∏ gpt: {err}")
        await message.answer(cmd_message.error_message)


@logger.catch
@router.message(F.text == "–°–±—Ä–æ—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
async def reset_context(message: Message, state: FSMContext):
    telegram_id = message.from_user.id
    try:
        # –û—á–∏—â–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        await clear_message_history(telegram_id)
        await message.reply(cmd_message.reset_context_message)
    except Exception as err:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±—Ä–æ—Å–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {err}")
        await message.answer(cmd_message.error_message)


@logger.catch
@router.message(F.text.in_(["‚ùå–ú–æ–¥–µ–ª—å 4-o‚ùå", "‚úÖ–ú–æ–¥–µ–ª—å 4-o-mini‚úÖ"]))
async def select_model(message: Message, state: FSMContext):
    model_mapping = {
        "‚ùå–ú–æ–¥–µ–ª—å 4-o‚ùå": "gpt-4o",
        "‚úÖ–ú–æ–¥–µ–ª—å 4-o-mini‚úÖ": "gpt-4o-mini"
    }
    model = model_mapping.get(message.text)
    
    await state.update_data(model=model)
    await state.set_state(Generate.text_input)

    await message.answer(f"–í—ã –≤—ã–±—Ä–∞–ª–∏ {model}. –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:", reply_markup=await kb.change_model(model))


@logger.catch
@router.message(Generate.text_input)
@count_calls()
async def process_generation(message: Message, state: FSMContext, bot: Bot):

    await bot.send_chat_action(message.chat.id, "typing")

    telegram_id = message.from_user.id
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
    if not await check_time_spacing_between_messages(telegram_id):
        # –ï—Å–ª–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –º–µ–Ω—å—à–µ 0.5 —Å–µ–∫—É–Ω–¥, –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
        return

    data = await state.get_data()
    model = data.get("model")
    user_input = message.text

    if model == "gpt-4o" and telegram_id != 857805093:
        await message.reply(f"–ú–æ–¥–µ–ª—å gpt-4o –≤ —Ä–µ–∂–∏–º–µ –∞–ª—å—Ñ–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å gpt-4o-mini")
        return
    
    lenght_message_user = calculate_message_length(user_input)

    if lenght_message_user >= 4096:
        await message.answer(f"–°–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–æ–∫—Ä–∞—Ç–∏—Ç–µ –µ–≥–æ –¥–ª–∏–Ω—É –¥–æ 4096 —Å–∏–º–≤–æ–ª–æ–≤.\n\n–î–ª–∏–Ω–∞ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è: {lenght_message_user}")
        await state.set_state(Generate.text_input)
        return

    current_state = await state.get_state()

    if current_state == Generate.waiting_for_response.state:
        await message.reply("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–∂–¥–∏—Ç–µ—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.")
        return

    await state.set_state(Generate.waiting_for_response)

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –æ–∂–∏–¥–∞–Ω–∏–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ ID
    waiting_message = await message.reply(f"‚ú® –ú–æ–¥–µ–ª—å: {model}. –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è: –≤—Å–µ–≥–æ 5-19 —Å–µ–∫—É–Ω–¥! ‚è±üöÄ\n–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ‚ú®")

    try:
        await bot.send_chat_action(message.chat.id, "typing")
        response = await gpt(user_input, model, telegram_id)
        response = escape_markdown(response)
    except Exception as err:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ gpt: {err}")
        await message.answer(cmd_message.error_message)
        await state.set_state(Generate.text_input)
        return

    try:
        response_parts = split_text(response)
        first_part = response_parts[0]
        await bot.edit_message_text(
            chat_id=waiting_message.chat.id,
            message_id=waiting_message.message_id,
            text=first_part,
            parse_mode="MarkdownV2"
        )
        if telegram_id == 857805093:
            await message.answer(
                f"Model: {model}\nNumber of tokens per input: {count_tokens(user_input)}\nNumber of tokens per output: {count_tokens(first_part)}"
                )
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —á–∞—Å—Ç–∏ (–µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å) –Ω–æ–≤—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
        for part in response_parts[1:]:
            await message.reply(
                part, 
                parse_mode="MarkdownV2"
            )
            if telegram_id == 857805093:
                await message.answer(
                    f"Model: {model}\nNumber of tokens per input: {count_tokens(user_input)}\nNumber of tokens per output: {count_tokens(part)}"
                    )
        logger.info(f"–û—Ç–≤–µ—Ç gpt –ø–æ–ª—É—á–µ–Ω –∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é: {telegram_id}")
        await state.set_state(Generate.text_input)
        await del_redis_id(telegram_id)
    except Exception as err:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {err}")
        await message.reply(cmd_message.error_message)
        await state.set_state(Generate.text_input)
        return


@logger.catch
@router.message(F.text)
async def error_handling(message: Message, state: FSMContext):
    current_state = await state.get_state()
    if current_state == Generate.waiting_for_response.state:
        await message.reply("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–∂–¥–∏—Ç–µ—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.")
    elif current_state == Generate.text_input.state:
        await process_generation(message, state)
    else:
        await message.answer("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å gpt", reply_markup=kb.main)
